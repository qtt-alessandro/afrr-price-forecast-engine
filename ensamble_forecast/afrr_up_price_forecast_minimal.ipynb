{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly_resampler import FigureResampler, FigureWidgetResampler\n",
    "\n",
    "from utils.afrr_preprocessing import preprocess_afrr_data\n",
    "from ensamble_forecast.lr_forecaster import run_lr_pipeline\n",
    "from ensamble_forecast.gp_forecaster import run_gp_pipeline\n",
    "from ensamble_forecast.xgb_forecaster import run_xgb_pipeline\n",
    "\n",
    "from utils.forecast_utils import get_forecast_params\n",
    "from darts.timeseries import concatenate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/afrr_price.parquet\"\n",
    "train_start = \"2024-10-01 22:00:00\"\n",
    "test_start = \"2025-01-09 22:00:00\"\n",
    "test_end = \"2025-03-20 22:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_params = get_forecast_params()\n",
    "output_chunk_length = forecast_params['output_chunk_length']\n",
    "forecast_horizon = forecast_params['forecast_horizon']\n",
    "stride = forecast_params['stride']\n",
    "quantiles = forecast_params['quantiles']\n",
    "\n",
    "# Run the pipeline with all parameters defined at the end\n",
    "lr_models, lr_results = run_lr_pipeline(\n",
    "    data_path=data_path,\n",
    "    hyper_params_path= \"../data/results/lr_hp_results.json\",\n",
    "    train_start=train_start,\n",
    "    test_start=test_start,\n",
    "    test_end=test_end,\n",
    "    output_chunk_length=output_chunk_length,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    stride=stride,\n",
    "    quantiles=quantiles\n",
    ")\n",
    "lr_results.to_parquet(\"lr_model_forecast.par\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian process Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_params = get_forecast_params()\n",
    "output_chunk_length = forecast_params['output_chunk_length']\n",
    "forecast_horizon = forecast_params['forecast_horizon']\n",
    "stride = forecast_params['stride']\n",
    "quantiles = forecast_params['quantiles']\n",
    "\n",
    "# Run the pipeline with all parameters defined at the end\n",
    "gp_model, gp_results = run_gp_pipeline(\n",
    "    data_path=data_path,\n",
    "    hyper_params_path=\"../data/results/gp_hp_results.json\",\n",
    "    train_start=train_start,\n",
    "    test_start=test_start,\n",
    "    test_end=test_end,\n",
    "    output_chunk_length=output_chunk_length,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    stride=stride,\n",
    "    quantiles=quantiles\n",
    ")\n",
    "\n",
    "gp_results.to_parquet(\"gp_model_forecast.par\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XgBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "EMBEDDING SUMMARY:\n",
      "Embedding dimension\t:  120\n",
      "Trajectory dimensions\t: (120, 4390)\n",
      "Complete dimension\t: (120, 4390)\n",
      "Missing dimension     \t: (120, 0)\n",
      "----------------------------------------\n",
      "DECOMPOSITION SUMMARY:\n",
      "Rank of trajectory\t\t: 120\n",
      "Dimension of projection space\t: 120\n",
      "Characteristic of projection\t: 1.0\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[14:23:38] /Users/runner/work/xgboost/xgboost/src/objective/quantile_obj.cu:39: Check failed: info.labels.Shape(1) == 1 (24 vs. 1) : Multi-target is not yet supported by the quantile loss.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000306638428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x00000003068b1594 xgboost::obj::QuantileRegression::Targets(xgboost::MetaInfo const&) const + 460\n  [bt] (2) 3   libxgboost.dylib                    0x00000003068334b0 xgboost::LearnerConfiguration::ConfigureTargets() + 368\n  [bt] (3) 4   libxgboost.dylib                    0x00000003068312ec xgboost::LearnerConfiguration::ConfigureModelParamWithoutBaseScore() + 32\n  [bt] (4) 5   libxgboost.dylib                    0x0000000306823570 xgboost::LearnerConfiguration::Configure() + 1308\n  [bt] (5) 6   libxgboost.dylib                    0x000000030682379c xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 128\n  [bt] (6) 7   libxgboost.dylib                    0x000000030665ab34 XGBoosterUpdateOneIter + 144\n  [bt] (7) 8   libffi.8.dylib                      0x000000010608004c ffi_call_SYSV + 76\n  [bt] (8) 9   libffi.8.dylib                      0x000000010607d834 ffi_call_int + 1404\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m stride \u001b[38;5;241m=\u001b[39m forecast_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Run the pipeline with all parameters defined at the end\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m xgb_model, xgb_results \u001b[38;5;241m=\u001b[39m run_xgb_pipeline(\n\u001b[1;32m      9\u001b[0m     data_path\u001b[38;5;241m=\u001b[39mdata_path,\n\u001b[1;32m     10\u001b[0m     hyper_params_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/results/xgb_hp_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     train_start\u001b[38;5;241m=\u001b[39mtrain_start,\n\u001b[1;32m     12\u001b[0m     test_start\u001b[38;5;241m=\u001b[39mtest_start,\n\u001b[1;32m     13\u001b[0m     test_end\u001b[38;5;241m=\u001b[39mtest_end,\n\u001b[1;32m     14\u001b[0m     output_chunk_length\u001b[38;5;241m=\u001b[39moutput_chunk_length,\n\u001b[1;32m     15\u001b[0m     forecast_horizon\u001b[38;5;241m=\u001b[39mforecast_horizon,\n\u001b[1;32m     16\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride)\n",
      "File \u001b[0;32m~/Desktop/afrr_price_ts_forecast/ensamble_forecast/xgb_forecaster.py:115\u001b[0m, in \u001b[0;36mrun_xgb_pipeline\u001b[0;34m(data_path, hyper_params_path, train_start, test_start, test_end, output_chunk_length, forecast_horizon, stride)\u001b[0m\n\u001b[1;32m    112\u001b[0m xgb_opt_params \u001b[38;5;241m=\u001b[39m load_hyperparameters(file_path\u001b[38;5;241m=\u001b[39mhyper_params_path)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Train XGBoost model\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m train_xgb_model(\n\u001b[1;32m    116\u001b[0m     afrr_pr_ts_scl_train, \n\u001b[1;32m    117\u001b[0m     exog_ts_scl_train, \n\u001b[1;32m    118\u001b[0m     xgb_opt_params, \n\u001b[1;32m    119\u001b[0m     output_chunk_length\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Generate forecasts\u001b[39;00m\n\u001b[1;32m    123\u001b[0m xgb_results \u001b[38;5;241m=\u001b[39m generate_xgb_forecasts(\n\u001b[1;32m    124\u001b[0m     xgb_model,\n\u001b[1;32m    125\u001b[0m     afrr_pr_ts_scl_train, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m     afrr_pr_scaler\n\u001b[1;32m    132\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/afrr_price_ts_forecast/ensamble_forecast/xgb_forecaster.py:37\u001b[0m, in \u001b[0;36mtrain_xgb_model\u001b[0;34m(afrr_pr_ts_scl_train, exog_ts_scl_train, xgb_params, output_chunk_length)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Create XGBoost model with the loaded parameters\u001b[39;00m\n\u001b[1;32m     22\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m XGBModel(\n\u001b[1;32m     23\u001b[0m     lags\u001b[38;5;241m=\u001b[39mxgb_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlags\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m     lags_past_covariates\u001b[38;5;241m=\u001b[39mxgb_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlags_past_covariates\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# quantile_alpha=0.5,  # For median regression\u001b[39;00m\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mfit(afrr_pr_ts_scl_train, past_covariates\u001b[38;5;241m=\u001b[39mexog_ts_scl_train)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xgb_model\n",
      "File \u001b[0;32m~/miniconda3/envs/opt_env/lib/python3.12/site-packages/darts/models/forecasting/xgboost.py:303\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, max_samples_per_ts, n_jobs_multioutput_wrapper, sample_weight, val_sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_container[quantile] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    304\u001b[0m     series\u001b[38;5;241m=\u001b[39mseries,\n\u001b[1;32m    305\u001b[0m     past_covariates\u001b[38;5;241m=\u001b[39mpast_covariates,\n\u001b[1;32m    306\u001b[0m     future_covariates\u001b[38;5;241m=\u001b[39mfuture_covariates,\n\u001b[1;32m    307\u001b[0m     val_series\u001b[38;5;241m=\u001b[39mval_series,\n\u001b[1;32m    308\u001b[0m     val_past_covariates\u001b[38;5;241m=\u001b[39mval_past_covariates,\n\u001b[1;32m    309\u001b[0m     val_future_covariates\u001b[38;5;241m=\u001b[39mval_future_covariates,\n\u001b[1;32m    310\u001b[0m     max_samples_per_ts\u001b[38;5;241m=\u001b[39mmax_samples_per_ts,\n\u001b[1;32m    311\u001b[0m     n_jobs_multioutput_wrapper\u001b[38;5;241m=\u001b[39mn_jobs_multioutput_wrapper,\n\u001b[1;32m    312\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    313\u001b[0m     val_sample_weight\u001b[38;5;241m=\u001b[39mval_sample_weight,\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/opt_env/lib/python3.12/site-packages/darts/models/forecasting/regression_model.py:943\u001b[0m, in \u001b[0;36mRegressionModel.fit\u001b[0;34m(self, series, past_covariates, future_covariates, max_samples_per_ts, n_jobs_multioutput_wrapper, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(component_lags_error_msg) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    941\u001b[0m     raise_log(\u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(component_lags_error_msg)), logger)\n\u001b[0;32m--> 943\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_model(\n\u001b[1;32m    944\u001b[0m     series\u001b[38;5;241m=\u001b[39mseries,\n\u001b[1;32m    945\u001b[0m     past_covariates\u001b[38;5;241m=\u001b[39mpast_covariates,\n\u001b[1;32m    946\u001b[0m     future_covariates\u001b[38;5;241m=\u001b[39mfuture_covariates,\n\u001b[1;32m    947\u001b[0m     val_series\u001b[38;5;241m=\u001b[39mval_series,\n\u001b[1;32m    948\u001b[0m     val_past_covariates\u001b[38;5;241m=\u001b[39mval_past_covariates,\n\u001b[1;32m    949\u001b[0m     val_future_covariates\u001b[38;5;241m=\u001b[39mval_future_covariates,\n\u001b[1;32m    950\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    951\u001b[0m     val_sample_weight\u001b[38;5;241m=\u001b[39mval_sample_weight,\n\u001b[1;32m    952\u001b[0m     max_samples_per_ts\u001b[38;5;241m=\u001b[39mmax_samples_per_ts,\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    954\u001b[0m )\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/opt_env/lib/python3.12/site-packages/darts/models/forecasting/regression_model.py:731\u001b[0m, in \u001b[0;36mRegressionModel._fit_model\u001b[0;34m(self, series, past_covariates, future_covariates, max_samples_per_ts, sample_weight, val_series, val_past_covariates, val_future_covariates, val_sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    728\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sample_weight` was ignored since underlying regression model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    729\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fit()` method does not support it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m         )\n\u001b[0;32m--> 731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    732\u001b[0m     training_samples, training_labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msample_weight_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    733\u001b[0m )\n\u001b[1;32m    735\u001b[0m \u001b[38;5;66;03m# generate and store the lagged components names (for feature importance analysis)\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lagged_feature_names, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lagged_label_names \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    737\u001b[0m     create_lagged_component_names(\n\u001b[1;32m    738\u001b[0m         target_series\u001b[38;5;241m=\u001b[39mseries,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    747\u001b[0m     )\n\u001b[1;32m    748\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/opt_env/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/opt_env/lib/python3.12/site-packages/xgboost/sklearn.py:1170\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1171\u001b[0m     params,\n\u001b[1;32m   1172\u001b[0m     train_dmatrix,\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1174\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1175\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1176\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1177\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1178\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1179\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1180\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1181\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1182\u001b[0m )\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/opt_env/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/opt_env/lib/python3.12/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/opt_env/lib/python3.12/site-packages/xgboost/core.py:2100\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2100\u001b[0m     _check_call(\n\u001b[1;32m   2101\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2103\u001b[0m         )\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/opt_env/lib/python3.12/site-packages/xgboost/core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [14:23:38] /Users/runner/work/xgboost/xgboost/src/objective/quantile_obj.cu:39: Check failed: info.labels.Shape(1) == 1 (24 vs. 1) : Multi-target is not yet supported by the quantile loss.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000306638428 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x00000003068b1594 xgboost::obj::QuantileRegression::Targets(xgboost::MetaInfo const&) const + 460\n  [bt] (2) 3   libxgboost.dylib                    0x00000003068334b0 xgboost::LearnerConfiguration::ConfigureTargets() + 368\n  [bt] (3) 4   libxgboost.dylib                    0x00000003068312ec xgboost::LearnerConfiguration::ConfigureModelParamWithoutBaseScore() + 32\n  [bt] (4) 5   libxgboost.dylib                    0x0000000306823570 xgboost::LearnerConfiguration::Configure() + 1308\n  [bt] (5) 6   libxgboost.dylib                    0x000000030682379c xgboost::LearnerImpl::UpdateOneIter(int, std::__1::shared_ptr<xgboost::DMatrix>) + 128\n  [bt] (6) 7   libxgboost.dylib                    0x000000030665ab34 XGBoosterUpdateOneIter + 144\n  [bt] (7) 8   libffi.8.dylib                      0x000000010608004c ffi_call_SYSV + 76\n  [bt] (8) 9   libffi.8.dylib                      0x000000010607d834 ffi_call_int + 1404\n\n"
     ]
    }
   ],
   "source": [
    "# Get forecast parameters\n",
    "forecast_params = get_forecast_params()\n",
    "output_chunk_length = forecast_params['output_chunk_length']\n",
    "forecast_horizon = forecast_params['forecast_horizon']\n",
    "stride = forecast_params['stride']\n",
    "\n",
    "# Run the pipeline with all parameters defined at the end\n",
    "xgb_model, xgb_results = run_xgb_pipeline(\n",
    "    data_path=data_path,\n",
    "    hyper_params_path=\"../data/results/xgb_hp_results.json\",\n",
    "    train_start=train_start,\n",
    "    test_start=test_start,\n",
    "    test_end=test_end,\n",
    "    output_chunk_length=output_chunk_length,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    stride=stride)\n",
    "\n",
    "#xgb_results.to_parquet(\"xgboost_model_forecast.par\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment check:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xgboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment check:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxgboost\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Darts \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdarts\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgboost' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"Environment check:\")\n",
    "    print(f\" Darts {darts.__version__}\")\n",
    "    \n",
    "    # Rest of your main code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version: 3.0.0\n",
      "Darts version: 0.34.0\n"
     ]
    }
   ],
   "source": [
    "# Add this at the start of your code to verify versions\n",
    "import xgboost, darts\n",
    "print(f\"XGBoost version: {xgboost.__version__}\")\n",
    "print(f\"Darts version: {darts.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(afrr_pr_ts_scl_train, \n",
    "afrr_pr_ts_scl_test, \n",
    "afrr_pr_ts_orig_train, \n",
    "afrr_pr_ts_orig_test, \n",
    "exog_ts_scl_train, \n",
    "exog_ts_scl_test,\n",
    "afrr_pr_scaler\n",
    ") = preprocess_afrr_data(data_path, train_start, test_start, test_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afrr_pr_ts_orig = concatenate([afrr_pr_ts_orig_train, afrr_pr_ts_orig_test], axis=0)\n",
    "afrr_pr_ts_orig = afrr_pr_ts_orig.with_columns_renamed(['aFRR_UpCapPriceEUR'], col_names_new=[\"afrr_up_cap_price\"])\n",
    "result_df = lr_results.join([gp_results, xgb_results, afrr_pr_ts_orig.pd_dataframe()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly_resampler import FigureResampler, FigureWidgetResampler\n",
    " \n",
    " \n",
    "fig = FigureWidgetResampler(go.Figure())\n",
    "fig.update_layout(margin=dict(l=10, r=10, t=10, b=10))\n",
    "fig.add_trace(go.Scattergl(name=r'AFRR Up Cap Price', showlegend=True), hf_x=result_df.index, hf_y=result_df['afrr_up_cap_price'])\n",
    "fig.add_trace(go.Scattergl(name=r'AFRR Up Cap Price Linear Model q = 0.5', showlegend=True), hf_x=result_df.index, hf_y=result_df['gp_afrr_up_cap_price_0.5'])\n",
    "fig.add_trace(go.Scattergl(name=r'AFRR Up Cap Price Gaussian Process Mean', showlegend=True), hf_x=result_df.index, hf_y=result_df['lr_afrr_up_cap_price_0.5'])\n",
    "fig.add_trace(go.Scattergl(name=r'AFRR Up Cap Price XGB Mean', showlegend=True), hf_x=result_df.index, hf_y=result_df['xgb_afrr_up_cap_price_0.5'])\n",
    "fig.update_layout(height=400, template=\"plotly_dark\")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
