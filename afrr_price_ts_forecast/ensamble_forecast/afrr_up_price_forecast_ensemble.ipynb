{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Jupyter magic commands\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly_resampler import FigureResampler, FigureWidgetResampler\n",
    "\n",
    "# Custom modules\n",
    "from ssa import *\n",
    "from gp_regressor import GPRegressor\n",
    "\n",
    "# Scikit-learn components\n",
    "from sklearn.gaussian_process.kernels import DotProduct, RBF, WhiteKernel\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from scipy import stats\n",
    "\n",
    "# Darts - Time Series components\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.datasets import AirPassengersDataset\n",
    "from darts.metrics import mape, rmse\n",
    "from darts.utils.missing_values import extract_subseries\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries as dt_attr\n",
    "\n",
    "# Darts - Models\n",
    "from darts.models import (\n",
    "    XGBModel,\n",
    "    ExponentialSmoothing,\n",
    "    LinearRegressionModel,\n",
    "    RandomForest,\n",
    "    RegressionModel,\n",
    "    TCNModel,\n",
    ")\n",
    "\n",
    "# Darts - Data processing\n",
    "from darts.dataprocessing import Pipeline\n",
    "from darts.dataprocessing.transformers import MissingValuesFiller, Scaler\n",
    "import json\n",
    "from afrr_preprocessing import preprocess_afrr_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m kernel \u001b[38;5;241m=\u001b[39m DotProduct() \u001b[38;5;241m+\u001b[39m WhiteKernel()\n\u001b[1;32m     14\u001b[0m gp_model \u001b[38;5;241m=\u001b[39m GPRegressor(\n\u001b[1;32m     15\u001b[0m     lags\u001b[38;5;241m=\u001b[39mgp_opt_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlags\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m     lags_past_covariates\u001b[38;5;241m=\u001b[39mgp_opt_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlags_past_covariates\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m     output_chunk_length\u001b[38;5;241m=\u001b[39moutput_chunk_length,\n\u001b[1;32m     18\u001b[0m     kernel\u001b[38;5;241m=\u001b[39mkernel)\n\u001b[0;32m---> 20\u001b[0m gp_model\u001b[38;5;241m.\u001b[39mfit(afrr_pr_ts_scl_train, past_covariates\u001b[38;5;241m=\u001b[39mexog_ts_scl_train)\n\u001b[1;32m     22\u001b[0m gp_backtest_forecasts \u001b[38;5;241m=\u001b[39m gp_model\u001b[38;5;241m.\u001b[39mhistorical_forecasts(\n\u001b[1;32m     23\u001b[0m     series\u001b[38;5;241m=\u001b[39mconcatenate([afrr_pr_ts_scl_train, afrr_pr_ts_scl_test], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     24\u001b[0m     past_covariates\u001b[38;5;241m=\u001b[39m concatenate([exog_ts_scl_train,exog_ts_scl_test], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     last_points_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m gp_backtest_forecasts \u001b[38;5;241m=\u001b[39m concatenate(gp_backtest_forecasts)\u001b[38;5;241m.\u001b[39mwith_columns_renamed(\n\u001b[1;32m     37\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maFRR_UpCapPriceEUR_cl_mu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maFRR_UpCapPriceEUR_cl_sigma\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     38\u001b[0m     col_names_new\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgp_afrr_up_cap_price_mu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgp_afrr_up_cap_price_sigma\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     39\u001b[0m )\u001b[38;5;241m.\u001b[39mpd_dataframe()\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/darts/models/forecasting/regression_model.py:948\u001b[0m, in \u001b[0;36mRegressionModel.fit\u001b[0;34m(self, series, past_covariates, future_covariates, max_samples_per_ts, n_jobs_multioutput_wrapper, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(component_lags_error_msg) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    946\u001b[0m     raise_log(\u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(component_lags_error_msg)), logger)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_model(\n\u001b[1;32m    949\u001b[0m     series\u001b[38;5;241m=\u001b[39mseries,\n\u001b[1;32m    950\u001b[0m     past_covariates\u001b[38;5;241m=\u001b[39mpast_covariates,\n\u001b[1;32m    951\u001b[0m     future_covariates\u001b[38;5;241m=\u001b[39mfuture_covariates,\n\u001b[1;32m    952\u001b[0m     val_series\u001b[38;5;241m=\u001b[39mval_series,\n\u001b[1;32m    953\u001b[0m     val_past_covariates\u001b[38;5;241m=\u001b[39mval_past_covariates,\n\u001b[1;32m    954\u001b[0m     val_future_covariates\u001b[38;5;241m=\u001b[39mval_future_covariates,\n\u001b[1;32m    955\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    956\u001b[0m     val_sample_weight\u001b[38;5;241m=\u001b[39mval_sample_weight,\n\u001b[1;32m    957\u001b[0m     max_samples_per_ts\u001b[38;5;241m=\u001b[39mmax_samples_per_ts,\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    959\u001b[0m )\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/darts/models/forecasting/regression_model.py:720\u001b[0m, in \u001b[0;36mRegressionModel._fit_model\u001b[0;34m(self, series, past_covariates, future_covariates, max_samples_per_ts, sample_weight, val_series, val_past_covariates, val_future_covariates, val_sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    717\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`sample_weight` was ignored since underlying regression model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    718\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fit()` method does not support it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    719\u001b[0m         )\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    721\u001b[0m     training_samples, training_labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msample_weight_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# generate and store the lagged components names (for feature importance analysis)\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lagged_feature_names, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lagged_label_names \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    726\u001b[0m     create_lagged_component_names(\n\u001b[1;32m    727\u001b[0m         target_series\u001b[38;5;241m=\u001b[39mseries,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    736\u001b[0m     )\n\u001b[1;32m    737\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpr.py:308\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood(theta, clone_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# First optimize starting from theta specified in kernel\u001b[39;00m\n\u001b[1;32m    306\u001b[0m optima \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    307\u001b[0m     (\n\u001b[0;32m--> 308\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constrained_optimization(\n\u001b[1;32m    309\u001b[0m             obj_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_\u001b[38;5;241m.\u001b[39mtheta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_\u001b[38;5;241m.\u001b[39mbounds\n\u001b[1;32m    310\u001b[0m         )\n\u001b[1;32m    311\u001b[0m     )\n\u001b[1;32m    312\u001b[0m ]\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Additional runs are performed from log-uniform chosen initial\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# theta\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_restarts_optimizer \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpr.py:653\u001b[0m, in \u001b[0;36mGaussianProcessRegressor._constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constrained_optimization\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj_func, initial_theta, bounds):\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin_l_bfgs_b\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 653\u001b[0m         opt_res \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39moptimize\u001b[38;5;241m.\u001b[39mminimize(\n\u001b[1;32m    654\u001b[0m             obj_func,\n\u001b[1;32m    655\u001b[0m             initial_theta,\n\u001b[1;32m    656\u001b[0m             method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    657\u001b[0m             jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    658\u001b[0m             bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[1;32m    659\u001b[0m         )\n\u001b[1;32m    660\u001b[0m         _check_optimize_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, opt_res)\n\u001b[1;32m    661\u001b[0m         theta_opt, func_min \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/scipy/optimize/_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    714\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:309\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    306\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# _prepare_scalar_function can use bounds=None to represent no bounds\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[38;5;241m=\u001b[39mjac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    310\u001b[0m                               bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[1;32m    311\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step)\n\u001b[1;32m    313\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    315\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:402\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    398\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    403\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:78\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_if_needed(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:72\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 72\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpr.py:298\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit.<locals>.obj_func\u001b[0;34m(theta, eval_gradient)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(theta, eval_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_gradient:\n\u001b[0;32m--> 298\u001b[0m         lml, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood(\n\u001b[1;32m    299\u001b[0m             theta, eval_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clone_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    300\u001b[0m         )\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mlml, \u001b[38;5;241m-\u001b[39mgrad\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/sklearn/gaussian_process/_gpr.py:640\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.log_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[1;32m    629\u001b[0m inner_term \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m K_inv[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Since we are interested about the trace of\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# inner_term @ K_gradient, we don't explicitly compute the\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# matrix-by-matrix operation and instead use an einsum. Therefore\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m#             K_gradient[..., param_idx]\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m#         )\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m log_likelihood_gradient_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mijl,jik->kl\u001b[39m\u001b[38;5;124m\"\u001b[39m, inner_term, K_gradient\n\u001b[1;32m    642\u001b[0m )\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# the log likehood gradient is the sum-up across the outputs\u001b[39;00m\n\u001b[1;32m    644\u001b[0m log_likelihood_gradient \u001b[38;5;241m=\u001b[39m log_likelihood_gradient_dims\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/opt_env/lib/python3.12/site-packages/numpy/core/einsumfunc.py:1001\u001b[0m, in \u001b[0;36m_einsum_dispatcher\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m     path \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meinsum_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m path\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (path, path_print)\n\u001b[0;32m-> 1001\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_einsum_dispatcher\u001b[39m(\u001b[38;5;241m*\u001b[39moperands, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;66;03m# Arguably we dispatch on more arguments than we really should; see note in\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;66;03m# _einsum_path_dispatcher for why.\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m operands\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(\n",
    "    afrr_pr_ts_scl_train, \n",
    "    afrr_pr_ts_scl_test, \n",
    "    afrr_pr_ts_orig_train, \n",
    "    afrr_pr_ts_orig_test, \n",
    "    exog_ts_scl_train, \n",
    "    exog_ts_scl_test,\n",
    "    afrr_pr_scaler\n",
    ") = preprocess_afrr_data(data_path=\"../data/afrr_price.parquet\")\n",
    "\n",
    "\n",
    "# Load the JSON file with the best parameters\n",
    "with open('results/lr_hp_results.json', 'r') as f:\n",
    "    lr_hyper_opt_params_dict = json.load(f)\n",
    "    \n",
    "with open('results/gp_hp_results.json', 'r') as f:\n",
    "    gp_hyper_opt_params_dict = json.load(f)\n",
    "\n",
    "# Extract the parameters from the loaded dictionary\n",
    "lr_opt_params = lr_hyper_opt_params_dict[\"parameters\"]\n",
    "gp_opt_params = gp_hyper_opt_params_dict[\"parameters\"]\n",
    "\n",
    "\n",
    "output_chunk_length = 24\n",
    "forecast_horizon = 24 \n",
    "stride = 24           \n",
    "start_idx = forecast_horizon \n",
    "quantiles = [0.1, 0.5, 0.9] \n",
    "\n",
    "\n",
    "### Models Definition\n",
    "\n",
    "# ---------------- Gaussian Process Model ----------------\n",
    "\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "    \n",
    "gp_model = GPRegressor(\n",
    "    lags=gp_opt_params[\"lags\"],\n",
    "    lags_past_covariates=gp_opt_params[\"lags_past_covariates\"],\n",
    "    output_chunk_length=output_chunk_length,\n",
    "    kernel=kernel)\n",
    "\n",
    "gp_model.fit(afrr_pr_ts_scl_train, past_covariates=exog_ts_scl_train)\n",
    "\n",
    "gp_backtest_forecasts = gp_model.historical_forecasts(\n",
    "    series=concatenate([afrr_pr_ts_scl_train, afrr_pr_ts_scl_test], axis=0),\n",
    "    past_covariates= concatenate([exog_ts_scl_train,exog_ts_scl_test], axis=0),\n",
    "    start=1,  \n",
    "    forecast_horizon=forecast_horizon,\n",
    "    enable_optimization=True,\n",
    "    num_samples=1,\n",
    "    predict_likelihood_parameters=True,    \n",
    "    stride=stride,\n",
    "    retrain=False,\n",
    "    last_points_only=False,\n",
    "    verbose=True)\n",
    "\n",
    "\n",
    "gp_backtest_forecasts = concatenate(gp_backtest_forecasts).with_columns_renamed(\n",
    "    ['aFRR_UpCapPriceEUR_cl_mu', 'aFRR_UpCapPriceEUR_cl_sigma'],\n",
    "    col_names_new=['gp_afrr_up_cap_price_mu', 'gp_afrr_up_cap_price_sigma']\n",
    ").pd_dataframe()\n",
    "\n",
    "\n",
    "gp_df_hat = pd.DataFrame(index=gp_backtest_forecasts.index)\n",
    "\n",
    "for q in quantiles:\n",
    "    gp_df_hat[f'gp_afrr_up_cap_price_{q}'] = afrr_pr_scaler.inverse_transform(\n",
    "        TimeSeries.from_series(\n",
    "            gp_backtest_forecasts['gp_afrr_up_cap_price_mu'] + \n",
    "            stats.norm.ppf(q) * gp_backtest_forecasts['gp_afrr_up_cap_price_sigma']\n",
    "        )\n",
    "    ).pd_series()\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- Linear Model with Quantiles ----------------\n",
    "\n",
    "lr_quantile_models = {}\n",
    "lr_backtest_forecasts = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    # Initialize QuantileRegressor for this quantile\n",
    "    quantile_regressor = QuantileRegressor(\n",
    "        alpha=0,\n",
    "        quantile=q,\n",
    "        solver='highs'\n",
    "    )\n",
    "    \n",
    "    # Create a specific model for this quantile\n",
    "    lr_quantile_models[q] = RegressionModel(\n",
    "        output_chunk_length=output_chunk_length,\n",
    "        lags=list(range(-1, -lr_opt_params[\"lags_max\"], -1)),\n",
    "        lags_past_covariates=list(range(-1, -lr_opt_params[\"lags_past_covariates_max\"], -1)), \n",
    "        model=quantile_regressor\n",
    "    )\n",
    "    \n",
    "    lr_quantile_models[q].fit(afrr_pr_ts_scl_train, past_covariates=exog_ts_scl_train)\n",
    "    \n",
    "    lr_backtest_forecasts[q] = lr_quantile_models[q].historical_forecasts(\n",
    "                                                                series=concatenate([afrr_pr_ts_scl_train, afrr_pr_ts_scl_test], axis=0),\n",
    "                                                                past_covariates=concatenate([exog_ts_scl_train, exog_ts_scl_test], axis=0),\n",
    "                                                                start=1,  \n",
    "                                                                forecast_horizon=forecast_horizon,\n",
    "                                                                enable_optimization=True,\n",
    "                                                                stride=stride,\n",
    "                                                                retrain=False,\n",
    "                                                                last_points_only=False,\n",
    "                                                                verbose=True\n",
    "                                                            )\n",
    "    \n",
    "dfs = [\n",
    "    afrr_pr_scaler.inverse_transform(\n",
    "        concatenate(lr_backtest_forecasts[q]).with_columns_renamed(\n",
    "            ['aFRR_UpCapPriceEUR_cl'], \n",
    "            col_names_new=[f'lr_afrr_up_cap_price_{q}']\n",
    "        )\n",
    "    ).pd_dataframe()\n",
    "    for q in [0.1, 0.5, 0.9]\n",
    "]\n",
    "\n",
    "lr_df_hat = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gp_forecaster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "EMBEDDING SUMMARY:\n",
      "Embedding dimension\t:  120\n",
      "Trajectory dimensions\t: (120, 3385)\n",
      "Complete dimension\t: (120, 3385)\n",
      "Missing dimension     \t: (120, 0)\n",
      "----------------------------------------\n",
      "DECOMPOSITION SUMMARY:\n",
      "Rank of trajectory\t\t: 120\n",
      "Dimension of projection space\t: 113\n",
      "Characteristic of projection\t: 0.9997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969b608a1f2447c4b05d968b6cfe2acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "historical forecasts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gp_model, gp_results = run_gp_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
